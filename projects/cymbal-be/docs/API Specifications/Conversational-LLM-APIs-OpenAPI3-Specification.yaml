openapi: 3.0.2
info:
  version: 1.0.0
  title: Conversational LLM Interaction APIs
  description: This API provides two LLM powered conversational interaction capabilities. First to receive conversation input for purpose of intent recognition, static Q&A based on KDB and dynamic customer profile information retrieval routing. Secondly to support multi-turn conversation for cases related to respond to customer queries on dynamic customer profile information. Made available under Apache-2.0 License https://www.apache.org/licenses/LICENSE-2.0

servers:
  - url: https://crbe-llm-vX-yyyyyyyyy-et.a.run.app

paths:
  /routeragent:
    post:
      operationId: getRouterAgentResponse
      description: Receive conversation input for purpose of intent recognition, static Q&A based on KDB and dynamic customer profile information retrieval routing. Returns depending on scenario if the question related to general FAQ or specific to customer profile information. If question is either classified as a SmallTalk or StaticKnowledgeBaseQnA senario, then an appropriate response in form of a small talk response or a static Q&A database based response is returned. If question is classified as a query specific to some customer profile information, the response indicates the customer profile scenario as a specific DynamicFlowAPI for which backend API information needs to be retrieved by the client and passed back to the chatllm
      security:
        - ApiKeyAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                llmRouterRequest:
                  description: The request payload from the client for which intent determination, routing and if applicable response needs to be provided. Max input token is 8192, max output token is 1024 for text bison, see this link https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models
                  type: string
                requestMetadata:
                  $ref: "#/components/schemas/requestMetadata"
                developerOptions:
                  $ref: "#/components/schemas/developerOptions"
                messages:
                  description: List of previous query / response paired messages from user and response messages from bot.Conversation history provided to the model in a structured alternate-author form. Messages appear in chronological order - oldest first, newest last. When the history of messages causes the input to exceed the maximum length, the oldest messages are removed until the entire prompt is within the allowed limit.
                  type: array
                  items:
                    description: Instance of previous query / response paired messages from user and response messages from bot
                    type: object
                    properties:
                      author:
                        description: Indicates who the message originated from - user (query) or bot (response)
                        type: string
                        enum:
                          - user
                          - bot                     
                      content:
                        description: Actual message from - user (query) or bot (response)
                        type: string                    
      responses:
        "200":
          description: Success response code for scenarios 1 - 4. The reponse can fall into 5 different scenarios. 1 > "smallTalk", this means the recognized query intent relates to making small talk and approriate responses are returned for the input query. 2 > "staticKnowledgebaseQnA", this means the reconized query intent relates to general Q&A on Cymbal products for which a response can be found in the FAQ Knowledge Base and appropriate responses are returned for the input query. 3 > "dynamicAPIFlow" , this means the recognized query intent relates to a question requiring specific customer information from the backend and reprocessing of the back end customer information along with the query in a second LLM API call to the getDynamicChatAgentResponse operation, the appropriate response indicated which type of customer information needs to be retieved from the backend system. 4 > "fallbackIntent" means query intent could not be recognized by the Router Agent, requiring appropriate fallback handling on client side. The fifth response description "llmFailure" will not be retured under code 200 but instead will be returned under code 400 or 500 since this deals with client or backend processing error responses 
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - smallTalk
                      - staticKnowledgebaseQnA
                      - dynamicAPIFlow
                      - fallbackIntent  
                  smallTalkResponse:
                    $ref: "#/components/schemas/smallTalkResponse"
                  originalUserQuery:
                    $ref: "#/components/schemas/originalUserQuery"
                  staticKnowledgebaseQnAResponse:
                    $ref: "#/components/schemas/staticKnowledgebaseQnAResponse"
                  dynamicAPIFlowResponse:
                    $ref: "#/components/schemas/dynamicAPIFlowResponse"
                  fallbackIntentResponse:
                    $ref: "#/components/schemas/fallbackIntentResponse"
        "400":
          description: Error response code for scenarios 5. < 5 > "llmFailure" means backend processing error occured on the API server side due to malformed or incomplete request and needs to be corrected on client side
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - llmFailure                
                  llmFailureResponse:
                    $ref: "#/components/schemas/llmFailureResponse"
        "500":
          description: Error response code for scenarios 5. < 5 > "llmFailure" means backend processing error occured on the API server side due to server side processing failure
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - llmFailure                  
                  llmFailureResponse:
                    $ref: "#/components/schemas/llmFailureResponse"

  /chatagent:
    post:
      operationId: getChatAgentResponse
      description: Receive conversation input for purpose of intent recognition and dynamic Q&A based on dynamic customer profile information passed in input prompt. Returns an appropriate conversational output based on prompt payload including prompt instructions and customer profile information. For multi turn conversations requiring conversation history tracking, client needs to pass previous chat request / responses in input as well.
      security:
        - ApiKeyAuth: []      
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                llmChatRequest:
                  $ref: "#/components/schemas/llmChatRequest"  
                requestMetadata:
                  $ref: "#/components/schemas/requestMetadata"
                developerOptions:
                  $ref: "#/components/schemas/developerOptions"
      responses:
        "200":
          description: Success response code when approriate responses are returned for the input query without any client or server side error
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - llmChatResponse
                  llmChatResponse:
                    $ref: "#/components/schemas/llmChatResponse"

        "400":
          description: Error response code for scenario "llmFailure" means backend processing error occured on the API server side due to malformed or incomplete request and needs to be corrected on client side
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - llmFailure                
                  llmFailureResponse:
                    $ref: "#/components/schemas/llmFailureResponse"
        "500":
          description: Error response code for scenario "llmFailure" means backend processing error occured on the API server side due to server side processing failure
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - llmFailure                  
                  llmFailureResponse:
                    $ref: "#/components/schemas/llmFailureResponse"

  /conversationalchatagent:
    post:
      operationId: getConversationalChatAgentResponse
      description: Receive conversation input for purpose of intent recognition and dynamic Q&A based on dynamic customer profile information passed in input prompt. Returns an appropriate conversational output based on prompt payload including prompt instructions and customer profile information. For multi turn conversations requiring conversation history tracking, client needs to pass previous chat request / responses in input as well.
      security:
        - ApiKeyAuth: []      
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                llmChatRequest:
                  $ref: "#/components/schemas/llmChatRequest"  
                requestMetadata:
                  $ref: "#/components/schemas/requestMetadata"
                developerOptions:
                  $ref: "#/components/schemas/developerOptions"
      responses:
        "200":
          description: Success response code when approriate responses are returned for the input query without any client or server side error
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - llmChatResponse
                  llmChatResponse:
                    $ref: "#/components/schemas/llmChatResponse"

        "400":
          description: Error response code for scenario "llmFailure" means backend processing error occured on the API server side due to malformed or incomplete request and needs to be corrected on client side
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - llmFailure                
                  llmFailureResponse:
                    $ref: "#/components/schemas/llmFailureResponse"
        "500":
          description: Error response code for scenario "llmFailure" means backend processing error occured on the API server side due to server side processing failure
          content:
            application/json:
              schema:
                type: object
                properties:
                  responseType:
                    description: The type of the response
                    type: string
                    enum:
                      - llmFailure                  
                  llmFailureResponse:
                    $ref: "#/components/schemas/llmFailureResponse"

components:
  schemas:
    requestMetadata:
      description: Transaction meta data attributes for client / session & user transaction correlation
      type: object
      properties:
        clientInfo:
          description: Metadata attribute identifying client. E.g. KATAAI
          type: string
        clientInfoAddnl:
          description: Additonal metadata attribute identifying client. E.g. Channel like MobileApp, Web etc if available for operational monitoring and debugging
          type: string
        sessionInfo:
          description: Metadata attribute identifying session E.g. Session ID on the client side to correlate log information
          type: string
        sessionInfoAddnl:
          description: Additonal metadata attribute identifying session. For Future Use
          type: string
        userInfo:
          description: Metadata attribute identifying user. E.g. MSISDN
          type: string
        userInfoAddnl:
          description: Additonal metadata attribute identifying user. For Future Use
          type: string

    developerOptions:
      description: Developer options to control & override specific server side configuration settings
      type: object
      properties:
        enableBasicLogging:
          description: Enable basic logging for this specific request. INFO logging will be enabled for this request on server side
          type: boolean
        enableAdvancedLogging:
          description: Enable additional logging for this specific request. DEBUG logging will be enabled for this request on server side
          type: boolean

    recognizedNLPEntities:
      description: The entities recognized from the user's query by the NLU system
      type: array
      items:
        type: object
        properties:
          entity:
            description: The name of the entity
            type: string
          value:
            description: The value of the entity
            type: string

    originalUserQuery:
      description: The original query from the user
      type: string

    smallTalkResponse:
      description: The response to the small talk input
      type: object
      properties:
        answer:
          description: The answer to the user's query
          type: string
        recognizedNLPEntities:
          $ref: "#/components/schemas/recognizedNLPEntities"      

    staticKnowledgebaseQnAResponse:
      description: The response from the knowledgebase collection
      type: object
      properties:
        topic:
          description: The main topic of the user's query
          type: string
        subTopic:
          description: The subtopic of the user's query
          type: string
        answer:
          description: The answer to the user's query
          type: string
        recognizedNLPEntities:
          $ref: "#/components/schemas/recognizedNLPEntities"    

    dynamicAPIFlowResponse:
      description: The type of dynamic API flow
      type: object
      properties:
        flowType:
          description: The type of dynamic API flow
          type: string
          enum:
            - accountInfo
            - billPaymentInfo
            - imPoinInfo
        subFlowType:
          description: The type of dynamic API sub flow. For future use
          type: string
        recognizedNLPEntities:
          $ref: "#/components/schemas/recognizedNLPEntities"

    fallbackIntentResponse:
      description: The response when the intent is not recognized
      type: object
      properties:
        fallBackReason:
          description: The reason for the fallback
          type: string
        fallBackResponse:
          description: The fallback response
          type: string
        recognizedNLPEntities:
          $ref: "#/components/schemas/recognizedNLPEntities"

    llmFailureResponse:
      description: The response when the LLM fails
      type: object
      properties:
        llmFailureErrorCode:
          description: The error code for the LLM failure
          type: string
        llmFailureReasonMsg:
          description: The reason for the LLM failure
          type: string
        llmFailureResponseMsg:
          description: The response for the LLM failure
          type: string

    llmChatRequest:
      description: The request payload from the client. This should contain (a) The prompt instructions for the model and associated payloads. Text input to generate model response. Prompts can include preamble, questions, suggestions, instructions, or examples and user profile information. (b) OPTIONALLY examples for finetuning output (c) OPTIONALLY previous request messages from user and response messages from bot (d) parameters to configure model. Max input token is 4096 (input token includes prompt, examples, previous request messages), max output token is 1024 for chat bison, see this link https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models
      type: object
      properties:
        context:
          description: The request payload from the client containing the ihe prompt instructions & associated payloads for the model
          type: string
        examples:
          description: The request payload from the client optionally containing list of example chat query input / response pairs. List of structured messages to the model to learn how to respond to the conversation.
          type: array
          items:
            description: Instance of example chat query input / response pair   
            type: object
            properties:
              input:
                description: Specific example chat query input 
                type: object
                properties:
                  content:
                    type: string
              output:
                description: Specific example chat query output                 
                type: object
                properties:
                  content:
                    type: string
        messages:
          description: List of previous query / response paired messages from user and response messages from bot.Conversation history provided to the model in a structured alternate-author form. Messages appear in chronological order - oldest first, newest last. When the history of messages causes the input to exceed the maximum length, the oldest messages are removed until the entire prompt is within the allowed limit.
          type: array
          items:
            description: Instance of previous query / response paired messages from user and response messages from bot
            type: object
            properties:
              author:
                description: Indicates who the message originated from - user (query) or bot (response)
                type: string
                enum:
                  - user
                  - bot                     
              content:
                description: Actual message from - user (query) or bot (response)
                type: string
        parameters:
          description: Additional parameters to optionally pass to LLM model
          type: object
          properties:
            temperature:
              description: The temperature is used for sampling during the response generation, which occurs when topP and topK are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic - the highest probability response is always selected. For most use cases, try starting with a temperature of 0.2. Should be 0.0 - 1.0, default 0
              type: number
            maxOutputTokens: 
              description: Maximum number of tokens that can be generated in the response. Specify a lower value for shorter responses and a higher value for longer responses. A token may be smaller than a word. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. May be 1–1024, default 128
              type: number
            topP:
              description: Top-p changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn't consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses. Should be 0.0 - 1.0, default 0.95
              type: number
            topK:
              description: Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses. Should be 1–40, Default- 40
              type: number
            modelOverride:
              description: For future use
              type: string

    llmChatResponse:
      description: The response from the model
      type: object
      properties:
        answer:
          description: The answer to the user's query
          type: string
        safetyAttributes:
          description: The safety attributes for the response
          type: object
          properties:
            scores:
              description: An array of scores for the response
              type: array
              items:
                type: number
            blocked:
              description: A boolean value that indicates whether the response is blocked
              type: boolean
            categories:
              description: An array of categories for the response
              type: array
              items:
                type: string

  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: x-api-key