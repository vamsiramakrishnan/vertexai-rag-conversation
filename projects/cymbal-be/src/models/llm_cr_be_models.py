# generated by fastapi-codegen:
#   filename:  GenAI-Sanbox-IOH/docs/API Specifications/IOH-Conversational-LLM-APIs-OpenAPI3-Specification.yaml
#   timestamp: 2023-05-22T11:11:42+00:00

from __future__ import annotations
from enum import Enum
from typing import List, Optional
from pydantic import BaseModel, Field


class RequestMetadata(BaseModel):
    clientInfo: Optional[str] = Field(
        None, description="Metadata attribute identifying client. E.g. KATAAI"
    )
    clientInfoAddnl: Optional[str] = Field(
        None,
        description="Additonal metadata attribute identifying client. E.g. Channel like MobileApp, Web etc if available for operational monitoring and debugging",
    )
    sessionInfo: Optional[str] = Field(
        None,
        description="Metadata attribute identifying session E.g. Session ID on the client side to correlate log information",
    )
    sessionInfoAddnl: Optional[str] = Field(
        None,
        description="Additonal metadata attribute identifying session. For Future Use",
    )
    userInfo: Optional[str] = Field(
        None, description="Metadata attribute identifying user. E.g. MSISDN"
    )
    userInfoAddnl: Optional[str] = Field(
        None,
        description="Additonal metadata attribute identifying user. For Future Use",
    )


class DeveloperOptions(BaseModel):
    enableBasicLogging: Optional[bool] = Field(
        None,
        description="Enable basic logging for this specific request. INFO logging will be enabled for this request on server side",
    )
    enableAdvancedLogging: Optional[bool] = Field(
        None,
        description="Enable additional logging for this specific request. DEBUG logging will be enabled for this request on server side",
    )


class RecognizedNLPEntity(BaseModel):
    entity: Optional[str] = Field(None, description="The name of the entity")
    value: Optional[str] = Field(None, description="The value of the entity")


class RecognizedNLPEntities(BaseModel):
    __root__: List[RecognizedNLPEntity] = Field(
        ...,
        description="The entities recognized from the user's query by the NLU system",
    )


class OriginalUserQuery(BaseModel):
    __root__: str = Field(..., description="The original query from the user")


class SmallTalkResponse(BaseModel):
    answer: Optional[str] = Field(None, description="The answer to the user's query")
    recognizedNLPEntities: Optional[RecognizedNLPEntities] = None


class StaticKnowledgebaseQnAResponse(BaseModel):
    topic: Optional[str] = Field(None, description="The main topic of the user's query")
    subTopic: Optional[str] = Field(
        None, description="The subtopic of the user's query"
    )
    answer: Optional[str] = Field(None, description="The answer to the user's query")
    recognizedNLPEntities: Optional[RecognizedNLPEntities] = None


class FlowType(Enum):
    accountInfo = "accountInfo"
    billPaymentInfo = "billPaymentInfo"
    imPoinInfo = "imPoinInfo"


class DynamicAPIFlowResponse(BaseModel):
    flowType: Optional[FlowType] = Field(
        None, description="The type of dynamic API flow"
    )
    subFlowType: Optional[str] = Field(
        None, description="The type of dynamic API sub flow. For future use"
    )
    recognizedNLPEntities: Optional[RecognizedNLPEntities] = None


class FallbackIntentResponse(BaseModel):
    fallBackReason: Optional[str] = Field(
        None, description="The reason for the fallback"
    )
    fallBackResponse: Optional[str] = Field(None, description="The fallback response")
    recognizedNLPEntities: Optional[RecognizedNLPEntities] = None


class LlmFailureResponse(BaseModel):
    llmFailureErrorCode: Optional[str] = Field(
        None, description="The error code for the LLM failure"
    )
    llmFailureReasonMsg: Optional[str] = Field(
        None, description="The reason for the LLM failure"
    )
    llmFailureResponseMsg: Optional[str] = Field(
        None, description="The response for the LLM failure"
    )


class Input(BaseModel):
    content: Optional[str] = None


class Output(BaseModel):
    content: Optional[str] = None


class Example(BaseModel):
    input: Optional[Input] = Field(
        None, description="Specific example chat query input"
    )
    output: Optional[Output] = Field(
        None, description="Specific example chat query output"
    )


class Author(Enum):
    user = "user"
    bot = "bot"


class Message(BaseModel):
    author: Optional[Author] = Field(
        None,
        description="Indicates who the message originated from - user (query) or bot (response)",
    )
    content: Optional[str] = Field(
        None, description="Actual message from - user (query) or bot (response)"
    )


class Parameters(BaseModel):
    temperature: Optional[float] = Field(
        None,
        description="The temperature is used for sampling during the response generation, which occurs when topP and topK are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic - the highest probability response is always selected. For most use cases, try starting with a temperature of 0.2. Should be 0.0 - 1.0, default 0",
    )
    maxOutputTokens: Optional[float] = Field(
        None,
        description="Maximum number of tokens that can be generated in the response. Specify a lower value for shorter responses and a higher value for longer responses. A token may be smaller than a word. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. May be 1–1024, default 128",
    )
    topP: Optional[float] = Field(
        None,
        description="Top-p changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn't consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses. Should be 0.0 - 1.0, default 0.95",
    )
    topK: Optional[float] = Field(
        None,
        description="Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses. Should be 1–40, Default- 40",
    )
    modelOverride: Optional[str] = Field(None, description="For future use")


class LlmChatRequest(BaseModel):
    context: Optional[str] = Field(
        None,
        description="The request payload from the client containing the ihe prompt instructions & associated payloads for the model",
    )
    examples: Optional[List[Example]] = Field(
        None,
        description="The request payload from the client optionally containing list of example chat query input / response pairs. List of structured messages to the model to learn how to respond to the conversation.",
    )
    messages: Optional[List[Message]] = Field(
        None,
        description="List of previous query / response paired messages from user and response messages from bot.Conversation history provided to the model in a structured alternate-author form. Messages appear in chronological order - oldest first, newest last. When the history of messages causes the input to exceed the maximum length, the oldest messages are removed until the entire prompt is within the allowed limit.",
    )
    parameters: Optional[Parameters] = Field(
        None, description="Additional parameters to optionally pass to LLM model"
    )


class SafetyAttributes(BaseModel):
    scores: Optional[List[float]] = Field(
        None, description="An array of scores for the response"
    )
    blocked: Optional[bool] = Field(
        None,
        description="A boolean value that indicates whether the response is blocked",
    )
    categories: Optional[List[str]] = Field(
        None, description="An array of categories for the response"
    )


class LlmChatResponse(BaseModel):
    answer: Optional[str] = Field(None, description="The answer to the user's query")
    safetyAttributes: Optional[SafetyAttributes] = Field(
        None, description="The safety attributes for the response"
    )


class RouterAgentPostRequest(BaseModel):
    llmRouterRequest: Optional[str] = Field(
        None,
        description="The request payload from the client for which intent determination, routing and if applicable response needs to be provided. Max input token is 8192, max output token is 1024 for text bison, see this link https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models",
    )
    requestMetadata: Optional[RequestMetadata] = None
    developerOptions: Optional[DeveloperOptions] = None
    messages: Optional[List[Message]] = Field(
        None,
        description="List of previous query / response paired messages from user and response messages from bot.Conversation history provided to the model in a structured alternate-author form. Messages appear in chronological order - oldest first, newest last. When the history of messages causes the input to exceed the maximum length, the oldest messages are removed until the entire prompt is within the allowed limit.",
    )    


class ResponseType(Enum):
    smallTalk = "smallTalk"
    staticKnowledgebaseQnA = "staticKnowledgebaseQnA"
    dynamicAPIFlow = "dynamicAPIFlow"
    fallbackIntent = "fallbackIntent"
    llmChatResponse = "llmChatResponse"


class RouterAgentPostResponse(BaseModel):
    responseType: Optional[ResponseType] = Field(
        None, description="The type of the response"
    )
    smallTalkResponse: Optional[SmallTalkResponse] = None
    originalUserQuery: Optional[OriginalUserQuery] = None
    staticKnowledgebaseQnAResponse: Optional[StaticKnowledgebaseQnAResponse] = None
    dynamicAPIFlowResponse: Optional[DynamicAPIFlowResponse] = None
    fallbackIntentResponse: Optional[FallbackIntentResponse] = None


class llmFailureResponseType(Enum):
    llmFailure = "llmFailure"


class RouterAgentErrorResponse(BaseModel):
    responseType: Optional[llmFailureResponseType] = Field(
        None, description="The type of the response"
    )
    llmFailureResponse: Optional[LlmFailureResponse] = None


class ChatAgentPostRequest(BaseModel):
    llmChatRequest: Optional[LlmChatRequest] = None
    requestMetadata: Optional[RequestMetadata] = None
    developerOptions: Optional[DeveloperOptions] = None


class ChatAgentPostResponse(BaseModel):
    responseType: Optional[ResponseType] = Field(
        None, description="The type of the response"
    )
    llmChatResponse: Optional[LlmChatResponse] = None


class ChatAgentErrorResponse(BaseModel):
    responseType: Optional[llmFailureResponseType] = Field(
        None, description="The type of the response"
    )
    llmFailureResponse: Optional[LlmFailureResponse] = None
