# This code was generated by fastapi-codegen:
#   filename:  GenAI-Sanbox-IOH/docs/API Specifications/cymbal-Conversational-LLM-APIs-OpenAPI3-Specification.yaml
#   timestamp: 2023-05-22T11:11:42+00:00

from __future__ import annotations
from enum import Enum
from typing import List, Optional
from pydantic import BaseModel, Field


# RequestMetadata class is used to store metadata information about the client, session and user.
class RequestMetadata(BaseModel):
    clientInfo: Optional[str] = Field(
        None, description="Metadata attribute identifying client. E.g. KATAAI"
    )
    clientInfoAddnl: Optional[str] = Field(
        None,
        description="Additonal metadata attribute identifying client. E.g. Channel like MobileApp, Web etc if available for operational monitoring and debugging",
    )
    sessionInfo: Optional[str] = Field(
        None,
        description="Metadata attribute identifying session E.g. Session ID on the client side to correlate log information",
    )
    sessionInfoAddnl: Optional[str] = Field(
        None,
        description="Additonal metadata attribute identifying session. For Future Use",
    )
    userInfo: Optional[str] = Field(
        None, description="Metadata attribute identifying user. E.g. MSISDN"
    )
    userInfoAddnl: Optional[str] = Field(
        None,
        description="Additonal metadata attribute identifying user. For Future Use",
    )


# DeveloperOptions class is used to enable basic and advanced logging for a specific request.
class DeveloperOptions(BaseModel):
    enableBasicLogging: Optional[bool] = Field(
        None,
        description="Enable basic logging for this specific request. INFO logging will be enabled for this request on server side",
    )
    enableAdvancedLogging: Optional[bool] = Field(
        None,
        description="Enable additional logging for this specific request. DEBUG logging will be enabled for this request on server side",
    )


# RecognizedNLPEntity class is used to store the name and value of the entity recognized by the NLU system.
class RecognizedNLPEntity(BaseModel):
    entity: Optional[str] = Field(None, description="The name of the entity")
    value: Optional[str] = Field(None, description="The value of the entity")


# RecognizedNLPEntities class is used to store the entities recognized by the NLU system.
class RecognizedNLPEntities(BaseModel):
    __root__: List[RecognizedNLPEntity] = Field(
        ...,
        description="The entities recognized from the user's query by the NLU system",
    )


# OriginalUserQuery class is used to store the original query from the user.
class OriginalUserQuery(BaseModel):
    __root__: str = Field(..., description="The original query from the user")


# SmallTalkResponse class is used to store the answer to the user's query and the entities recognized by the NLU system.
class SmallTalkResponse(BaseModel):
    answer: Optional[str] = Field(None, description="The answer to the user's query")
    recognizedNLPEntities: Optional[RecognizedNLPEntities] = None


# StaticKnowledgebaseQnAResponse class is used to store the main topic, subtopic, answer to the user's query and the entities recognized by the NLU system.
class StaticKnowledgebaseQnAResponse(BaseModel):
    topic: Optional[str] = Field(None, description="The main topic of the user's query")
    subTopic: Optional[str] = Field(
        None, description="The subtopic of the user's query"
    )
    answer: Optional[str] = Field(None, description="The answer to the user's query")
    recognizedNLPEntities: Optional[RecognizedNLPEntities] = None


# FlowType Enum is used to define the type of dynamic API flow.
class FlowType(Enum):
    accountInfo = "accountInfo"
    billPaymentInfo = "billPaymentInfo"
    imPoinInfo = "imPoinInfo"


# DynamicAPIFlowResponse class is used to store the type of dynamic API flow, the type of dynamic API sub flow and the entities recognized by the NLU system.
class DynamicAPIFlowResponse(BaseModel):
    flowType: Optional[FlowType] = Field(
        None, description="The type of dynamic API flow"
    )
    subFlowType: Optional[str] = Field(
        None, description="The type of dynamic API sub flow. For future use"
    )
    recognizedNLPEntities: Optional[RecognizedNLPEntities] = None


# FallbackIntentResponse class is used to store the reason for the fallback, the fallback response and the entities recognized by the NLU system.
class FallbackIntentResponse(BaseModel):
    fallBackReason: Optional[str] = Field(
        None, description="The reason for the fallback"
    )
    fallBackResponse: Optional[str] = Field(None, description="The fallback response")
    recognizedNLPEntities: Optional[RecognizedNLPEntities] = None


# LlmFailureResponse class is used to store the error code, the reason and the response for the LLM failure.
class LlmFailureResponse(BaseModel):
    llmFailureErrorCode: Optional[str] = Field(
        None, description="The error code for the LLM failure"
    )
    llmFailureReasonMsg: Optional[str] = Field(
        None, description="The reason for the LLM failure"
    )
    llmFailureResponseMsg: Optional[str] = Field(
        None, description="The response for the LLM failure"
    )


# Input class is used to store the content of the chat query.
class Input(BaseModel):
    content: Optional[str] = None


# Output class is used to store the content of the chat response.
class Output(BaseModel):
    content: Optional[str] = None


# Example class is used to store specific example chat query input and output.
class Example(BaseModel):
    input: Optional[Input] = Field(
        None, description="Specific example chat query input"
    )
    output: Optional[Output] = Field(
        None, description="Specific example chat query output"
    )


# Author Enum is used to indicate who the message originated from - user (query) or bot (response).
class Author(Enum):
    user = "user"
    bot = "bot"


# Message class is used to store the author and content of the message.
class Message(BaseModel):
    author: Optional[Author] = Field(
        None,
        description="Indicates who the message originated from - user (query) or bot (response)",
    )
    content: Optional[str] = Field(
        None, description="Actual message from - user (query) or bot (response)"
    )


# Parameters class is used to store additional parameters to optionally pass to LLM model.
class Parameters(BaseModel):
    temperature: Optional[float] = Field(
        None,
        description="The temperature is used for sampling during the response generation, which occurs when topP and topK are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic - the highest probability response is always selected. For most use cases, try starting with a temperature of 0.2. Should be 0.0 - 1.0, default 0",
    )
    maxOutputTokens: Optional[float] = Field(
        None,
        description="Maximum number of tokens that can be generated in the response. Specify a lower value for shorter responses and a higher value for longer responses. A token may be smaller than a word. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. May be 1–1024, default 128",
    )
    topP: Optional[float] = Field(
        None,
        description="Top-p changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn't consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses. Should be 0.0 - 1.0, default 0.95",
    )
    topK: Optional[float] = Field(
        None,
        description="Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses. Should be 1–40, Default- 40",
    )
    modelOverride: Optional[str] = Field(None, description="For future use")


# LlmChatRequest class is used to store the request payload from the client containing the ihe prompt instructions & associated payloads for the model, list of example chat query input / response pairs, list of previous query / response paired messages from user and response messages from bot and additional parameters to optionally pass to LLM model.
class LlmChatRequest(BaseModel):
    context: Optional[str] = Field(
        None,
        description="The request payload from the client containing the ihe prompt instructions & associated payloads for the model",
    )
    examples: Optional[List[Example]] = Field(
        None,
        description="The request payload from the client optionally containing list of example chat query input / response pairs. List of structured messages to the model to learn how to respond to the conversation.",
    )
    messages: Optional[List[Message]] = Field(
        None,
        description="List of previous query / response paired messages from user and response messages from bot.Conversation history provided to the model in a structured alternate-author form. Messages appear in chronological order - oldest first, newest last. When the history of messages causes the input to exceed the maximum length, the oldest messages are removed until the entire prompt is within the allowed limit.",
    )
    parameters: Optional[Parameters] = Field(
        None, description="Additional parameters to optionally pass to LLM model"
    )


# SafetyAttributes class is used to store an array of scores for the response, a boolean value that indicates whether the response is blocked and an array of categories for the response.
class SafetyAttributes(BaseModel):
    scores: Optional[List[float]] = Field(
        None, description="An array of scores for the response"
    )
    blocked: Optional[bool] = Field(
        None,
        description="A boolean value that indicates whether the response is blocked",
    )
    categories: Optional[List[str]] = Field(
        None, description="An array of categories for the response"
    )


# LlmChatResponse class is used to store the answer to the user's query and the safety attributes for the response.
class LlmChatResponse(BaseModel):
    answer: Optional[str] = Field(None, description="The answer to the user's query")
    safetyAttributes: Optional[SafetyAttributes] = Field(
        None, description="The safety attributes for the response"
    )


# RouterAgentPostRequest class is used to store the request payload from the client for which intent determination, routing and if applicable response needs to be provided, metadata information about the client, session and user and to enable basic and advanced logging for a specific request.
class RouterAgentPostRequest(BaseModel):
    llmRouterRequest: Optional[str] = Field(
        None,
        description="The request payload from the client for which intent determination, routing and if applicable response needs to be provided. Max input token is 8192, max output token is 1024 for text bison, see this link https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models",
    )
    requestMetadata: Optional[RequestMetadata] = None
    developerOptions: Optional[DeveloperOptions] = None
    messages: Optional[List[Message]] = Field(
        None,
        description="List of previous query / response paired messages from user and response messages from bot.Conversation history provided to the model in a structured alternate-author form. Messages appear in chronological order - oldest first, newest last. When the history of messages causes the input to exceed the maximum length, the oldest messages are removed until the entire prompt is within the allowed limit.",
    )    


# ResponseType Enum is used to define the type of the response.
class ResponseType(Enum):
    smallTalk = "smallTalk"
    staticKnowledgebaseQnA = "staticKnowledgebaseQnA"
    dynamicAPIFlow = "dynamicAPIFlow"
    fallbackIntent = "fallbackIntent"
    llmChatResponse = "llmChatResponse"


# RouterAgentPostResponse class is used to store the type of the response, the answer to the user's query, the original query from the user, the main topic, subtopic, answer to the user's query and the entities recognized by the NLU system, the type of dynamic API flow, the type of dynamic API sub flow and the entities recognized by the NLU system, the reason for the fallback, the fallback response and the entities recognized by the NLU system.
class RouterAgentPostResponse(BaseModel):
    responseType: Optional[ResponseType] = Field(
        None, description="The type of the response"
    )
    smallTalkResponse: Optional[SmallTalkResponse] = None
    originalUserQuery: Optional[OriginalUserQuery] = None
    staticKnowledgebaseQnAResponse: Optional[StaticKnowledgebaseQnAResponse] = None
    dynamicAPIFlowResponse: Optional[DynamicAPIFlowResponse] = None
    fallbackIntentResponse: Optional[FallbackIntentResponse] = None


# llmFailureResponseType Enum is used to define the type of the response.
class llmFailureResponseType(Enum):
    llmFailure = "llmFailure"


# RouterAgentErrorResponse class is used to store the type of the response and the error code, the reason and the response for the LLM failure.
class RouterAgentErrorResponse(BaseModel):
    responseType: Optional[llmFailureResponseType] = Field(
        None, description="The type of the response"
    )
    llmFailureResponse: Optional[LlmFailureResponse] = None


# ChatAgentPostRequest class is used to store the request payload from the client containing the ihe prompt instructions & associated payloads for the model, metadata information about the client, session and user and to enable basic and advanced logging for a specific request.
class ChatAgentPostRequest(BaseModel):
    llmChatRequest: Optional[LlmChatRequest] = None
    requestMetadata: Optional[RequestMetadata] = None
    developerOptions: Optional[DeveloperOptions] = None


# ChatAgentPostResponse class is used to store the type of the response and the answer to the user's query and the safety attributes for the response.
class ChatAgentPostResponse(BaseModel):
    responseType: Optional[ResponseType] = Field(
        None, description="The type of the response"
    )
    llmChatResponse: Optional[LlmChatResponse] = None


# ChatAgentErrorResponse class is used to store the type of the response and the error code, the reason and the response for the LLM failure.
class ChatAgentErrorResponse(BaseModel):
    responseType: Optional[llmFailureResponseType] = Field(
        None, description="The type of the response"
    )
    llmFailureResponse: Optional[LlmFailureResponse] = None

