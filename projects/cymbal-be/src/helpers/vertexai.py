from langchain.llms.vertexai import VertexAI
from langchain.chat_models.vertexai import ChatVertexAI, _parse_chat_history
#from helpers.logging_configurator import logger, request_origin
from helpers.logging_configurator import logger, request_origin, service_metrics
from typing import TYPE_CHECKING, Any, Dict, List, Optional
from pydantic import BaseModel, root_validator
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.embeddings import VertexAIEmbeddings

from langchain.chat_models.base import BaseChatModel
from langchain.llms.vertexai import _VertexAICommon, is_codey_model
from langchain.schema import (
    ChatGeneration,
    ChatResult,
)
from langchain.schema.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
)
from vertexai.language_models import TextEmbeddingInput


class LoggingVertexAI(VertexAI):
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        # Log the request
        logger.info(
            f"VertexLLMRequest - Origin: {request_origin.get()} | LLMPrompt: *** {prompt} *** | LLMArguments: {kwargs}"
        )
        logger.info(
            f"VertexLLMPayload - Origin: {request_origin.get()} | RequestPayload: {len(prompt)}"
        )
        service_metrics.get().setMetrics("MetricsVertexTextRequest", len(prompt))

        response = super()._call(prompt, stop, run_manager, **kwargs)

        # Log the response
        logger.info(
            f"VertexLLMResponse - Origin: {request_origin.get()} | LLMResponse: *** {response} ***"
        )
        logger.info(
            f"VertexLLMPayload - Origin: {request_origin.get()} | ResponsePayload: {len(response)}"
        )
        service_metrics.get().setMetrics("MetricsVertexTextResponse", len(response))

        return response


class LoggingVertexAIChat(ChatVertexAI):
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Generate next turn in the conversation.

        Args:
            messages: The history of the conversation as a list of messages. Code chat
                does not support context.
            stop: The list of stop words (optional).
            run_manager: The CallbackManager for LLM run, it's not used at the moment.

        Returns:
            The ChatResult that contains outputs generated by the model.

        Raises:
            ValueError: if the last message in the list is not from human.
        """
        if not messages:
            raise ValueError(
                "You should provide at least one message to start the chat!"
            )
        question = messages[-1]
        if not isinstance(question, HumanMessage):
            raise ValueError(
                f"Last message in the list should be from human, got {question.type}."
            )
        history = _parse_chat_history(messages[:-1])
        context = history.context if history.context else None
        params = {**self._default_params, **kwargs}
        examples = kwargs.get("examples", None)
        if examples:
            params["examples"] = _parse_examples(examples)
        if not self.is_codey_model:
            chat = self.client.start_chat(
                context=context, message_history=history.history, **params
            )
        else:
            chat = self.client.start_chat(**params)
        # Log the request
        logger.info(
            f"VertexLLMChatRequest - Origin: {request_origin.get()} | LLMPrompt: *** {context} *** | LLMQuestion: *** {question.content} *** | LLMArguments: {params}"
        )
        logger.info(
            f"VertexLLMChatPayload - Origin: {request_origin.get()} | RequestPayload: {len(context+question.content)}"
        )
        service_metrics.get().setMetrics("MetricsVertexChatRequest", len(context+question.content))                    
        # Log the request            
        response = chat.send_message(question.content)
        # Log the response
        logger.info(
            f"VertexLLMChatResponse - Origin: {request_origin.get()} | LLMResponse: *** {response.text} ***"
        )
        logger.info(
            f"VertexLLMChatPayload - Origin: {request_origin.get()} | ResponsePayload: {len(response.text)}"
        )        
        service_metrics.get().setMetrics("MetricsVertexChatResponse", len(response.text))        
        text = self._enforce_stop_words(response.text, stop)
        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])

class LoggingVertexAIEmbeddings(VertexAIEmbeddings):

    # ["RETRIEVAL_QUERY", "RETRIEVAL_DOCUMENT", "SEMANTIC_SIMILARITY", "CLASSIFICATION"
    task_type: str = "RETRIEVAL_QUERY" 

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def embed_query(self, text: str) -> List[float]:
        embeddings = super().embed_query(TextEmbeddingInput(task_type=self.task_type, text=text))
        logger.info(
            f"VertexEmbeddings - Origin: {request_origin.get()} | Embeddings Model Used {self.model_name} / {self.task_type}"
        )          
        return embeddings
